{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import imageio\n",
    "\n",
    "from FDA_Projection import FDA\n",
    "from NeuralSDE import NeuralSDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AMZN', 'IBM', 'INTC', 'TSLA'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"G:\\gitCode\\BU_programming\\\\finnal_project\\FuNVol\\\\self.test_data\\\\volatility surface.csv\")\n",
    "np.unique(raw_data['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data[['date', 'days', 'delta', 'impl_volatility', 'ticker']]\n",
    "raw_data = raw_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = np.unique(raw_data['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker name: AMZN, start time 2022-01-03 00:00:00, end time:2023-02-28 00:00:00\n",
      "IV data shape (290, 11, 17)\n",
      "ticker name: IBM, start time 2022-01-03 00:00:00, end time:2023-02-28 00:00:00\n",
      "IV data shape (290, 11, 17)\n",
      "ticker name: INTC, start time 2022-01-03 00:00:00, end time:2023-02-28 00:00:00\n",
      "IV data shape (290, 11, 17)\n",
      "ticker name: TSLA, start time 2022-01-03 00:00:00, end time:2023-02-28 00:00:00\n",
      "IV data shape (290, 11, 17)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"G:\\gitCode\\BU_programming\\\\finnal_project\\FuNVol\\\\self.test_data\\\\\"\n",
    "\n",
    "def daily_IV_matrix(df:pd.DataFrame, date:pd.DatetimeTZDtype):\n",
    "    return pd.pivot_table(df, values='impl_volatility', index='days', columns='delta').values\n",
    "    \n",
    "for ticker in tickers:\n",
    "\n",
    "    data_by_ticker = copy.deepcopy(raw_data[raw_data['ticker']==ticker])\n",
    "    data_by_ticker['date'] = pd.to_datetime(data_by_ticker['date'])\n",
    "    start = data_by_ticker['date'].min()\n",
    "    end = data_by_ticker['date'].max()\n",
    "    data_by_ticker.sort_values(['date', 'days', 'delta'])\n",
    "\n",
    "    price_df = pd.read_csv(data_path+\"{}.csv\".format(ticker))\n",
    "    price_df['date'] = pd.to_datetime(price_df['Date'])\n",
    "    price_df.sort_values('date', inplace=True)\n",
    "\n",
    "    print(\"ticker name: {}, start time {}, end time:{}\".format(ticker, start, end))\n",
    "    iv_by_day = np.array([daily_IV_matrix(df, date) for date, df in data_by_ticker.groupby(['date'])])\n",
    "    print(\"IV data shape {}\".format(iv_by_day.shape))\n",
    "    # iv_by_day.shape = (2893, 11, 17)\n",
    "    dates = np.sort(np.array(np.unique(data_by_ticker['date'])))\n",
    "    tau = np.sort(np.array(np.unique(data_by_ticker['days']))) / 365\n",
    "    Delta = np.sort(np.array(np.unique(data_by_ticker['delta']))) / 100\n",
    "    result_dict = {\n",
    "        'dates': dates,\n",
    "        'tau': tau,\n",
    "        'Delta': Delta,\n",
    "        'IV': iv_by_day,\n",
    "        'prices': price_df['Adj Close'].values\n",
    "    }\n",
    "    with open(data_path+\"raw_{}.pickle\".format(ticker), 'wb') as handle:\n",
    "        pickle.dump(result_dict, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import imageio\n",
    "class Prediction:\n",
    "\n",
    "    def __init__(self, data_dir, tickers, model, K=3) -> None:\n",
    "\n",
    "        #load all data\n",
    "        self.raw_data = []\n",
    "        self.raw_price_data = []\n",
    "        self.data = []\n",
    "        self.current_ticker=None\n",
    "        self.tickers = np.array(tickers)\n",
    "        self.trained_model = model\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        print(\"loading data for various tickers...\")\n",
    "        # load the data and normalize individually        \n",
    "        for ticker in tickers:\n",
    "            \n",
    "            # the raw data files cannot be provided due to licensing issues\n",
    "            with open(data_dir + \"raw_\" + ticker + '.pickle', 'rb') as handle:\n",
    "                self.raw_data.append(pickle.load(handle))\n",
    "\n",
    "        print(\"normalizing IVs...\")\n",
    "        # normalizing coefficients\n",
    "        self.nrm = []\n",
    "        self.reg = []\n",
    "\n",
    "        for data in copy.deepcopy(self.raw_data):\n",
    "            \n",
    "            a_, b_, IV = model.Normalise(np.log(np.exp(data['IV'])-1))\n",
    "            data['IV'] = np.where(np.isneginf(IV), np.NaN, IV)   # !!!!\n",
    "            \n",
    "            c_, d_, prices = model.Normalise(np.log(data['prices']))\n",
    "            data['prices'] = prices\n",
    "\n",
    "            self.nrm.append({'IV':[a_,b_], 'price' : [c_,d_]})\n",
    "            self.data.append(data)\n",
    "            \n",
    "        print(\"truncating data to common set of dates...\")\n",
    "        # only keep data on dates where all tickers have data and\n",
    "        # collate the data into a single source\n",
    "        self.data_all = {'dates' : [],\n",
    "                         'tau' : self.data[0]['tau'],\n",
    "                         'Delta' : self.data[0]['Delta'],\n",
    "                         'IV' : [],\n",
    "                         'prices' : [],\n",
    "                         'ticker_idx' : []}\n",
    "        \n",
    "        self.delta = self.data[0]['Delta']\n",
    "        self.tau = self.data[0]['tau']\n",
    "\n",
    "        for i, ticker in enumerate(tickers):\n",
    "            \n",
    "            # detrend the time serise data\n",
    "            alpha_, beta_, prices_d = model.Detrend(self.data[i]['prices'])\n",
    "            self.reg.append([alpha_,beta_])\n",
    "            self.data[i]['prices'] = prices_d\n",
    "            \n",
    "            if i ==0 :\n",
    "                self.data_all['IV'] = self.data[i]['IV']\n",
    "                self.data_all['dates'] = self.data[i]['dates']\n",
    "                self.data_all['ticker_idx'] = np.zeros(len(self.data[i]['dates']), int)\n",
    "                self.data_all['prices'] = self.data[i]['prices']\n",
    "                \n",
    "            else:\n",
    "                self.data_all['IV'] = np.concatenate((self.data_all['IV'], self.data[i]['IV']))\n",
    "                self.data_all['dates'] = np.concatenate((self.data_all['dates'], self.data[i]['dates']))\n",
    "                self.data_all['prices'] = np.concatenate((self.data_all['prices'], self.data[i]['prices']))\n",
    "                self.data_all['ticker_idx'] = np.concatenate((self.data_all['ticker_idx'], i + np.zeros(len(self.data[i]['dates']), int)))\n",
    "\n",
    "        self.rng_IV = np.nanquantile(self.data_all[\"IV\"].flatten(), [0.025, 0.975])\n",
    "        print(\"Project onto trained common FPCs\")\n",
    "        self.K = K        \n",
    "        self.b = []\n",
    "        self.b_all = np.zeros((0, K))\n",
    "        self.b_sim = []\n",
    "        self.b_price = []\n",
    "        \n",
    "        for data in self.data:\n",
    "            self.b.append(model.Perform_FPC_Projection(data['IV'], K))\n",
    "            self.b_all = np.concatenate((self.b_all, self.b[-1]))\n",
    "            self.b_price.append(np.concatenate((model.Perform_FPC_Projection(data['IV'], K),\n",
    "                                data['prices'].reshape(-1,1)), axis=1))  \n",
    "            \n",
    "            \n",
    "    \n",
    "        N = (self.b[0].shape[1]+1)   \n",
    "        self.current_ticker = self.tickers\n",
    "        # concatenate coefficients of FDA with prices across all tickers\n",
    "        for i in range(len(self.tickers)):\n",
    "            if i == 0:\n",
    "                data = np.concatenate((self.b[i][:,:N-1].reshape(-1,N-1), self.data[i]['prices'].reshape(-1,1)), axis=1)\n",
    "                dates = pd.to_datetime(self.data[i]['dates'], format='%Y%m%d')\n",
    "                T = (dates-dates[0])/ np.timedelta64(1, 'D')\n",
    "                T = T.values.reshape(-1,1)/365\n",
    "            else:\n",
    "                data = np.concatenate((data, self.b[i][:,:N-1].reshape(-1,N-1), self.data[i]['prices'].reshape(-1,1)), axis=1)\n",
    "        \n",
    "        self.n_lags = self.trained_model.neural_sde.n_lags\n",
    "        self.test_data = torch.from_numpy(self.trained_model.neural_sde.Normalize_Data(data)).float()\n",
    "            \n",
    "        simulated_IV_list = []\n",
    "        pred_days = 3\n",
    "        with tqdm(total=(self.test_data.shape[0]-9)*pred_days, desc='simulating IV........') as pdar:\n",
    "            for i in range(9, self.test_data.shape[0]):\n",
    "                    for j in range(1, pred_days+1):\n",
    "                        simulated_IV_list.append(self.simulate_Time_Series(dT=j/365,start=i))\n",
    "                        pdar.update(1)\n",
    "        self.simulated_df = pd.concat(simulated_IV_list, axis=0)\n",
    "            \n",
    "\n",
    "    def simulate_Time_Series(self, dT=1/365, start=9):\n",
    "        input_data = self.test_data[start-self.n_lags+1:start+1, :].unsqueeze(0).to('cuda:0')\n",
    "        predi_data = self.trained_model.neural_sde.pi_drift.forward(input_data).view(-1).to('cpu')\n",
    "        nu_pred = torch.add(input_data[0, -1, :].to('cpu'),(predi_data*dT)).to('cpu')\n",
    "\n",
    "        def Data_per_Ticker(b_sim, ticker_idx):\n",
    "            price = self.trained_model.reg[ticker_idx][0] + self.trained_model.reg[ticker_idx][1][0] * (self.trained_model.train_size+start+1) + b_sim[-1]\n",
    "            price = np.exp(self.trained_model.UnNormalise(self.trained_model.nrm[ticker_idx]['price'][0], self.trained_model.nrm[ticker_idx]['price'][1], price.detach().numpy())).reshape(-1)\n",
    "            b = np.array([[b_sim[:-1].detach().numpy()]])\n",
    "            delta_grid_t, tau_grid_t, IV_t = self.trained_model.fda_model.Generate_IV_Grid(b=b, delta = self.trained_model.x, tau = self.trained_model.y)\n",
    "            IV = np.log(1+ np.exp(self.trained_model.UnNormalise(self.trained_model.nrm[ticker_idx]['IV'][0], self.trained_model.nrm[ticker_idx]['IV'][1], IV_t)))\n",
    "            IV = IV.transpose(1,0,2,3)\n",
    "            \n",
    "            return delta_grid_t, tau_grid_t, IV, price\n",
    "        \n",
    "        seq_length = int(len(nu_pred)/len(self.tickers))\n",
    "        df_list = []\n",
    "        for i in range(len(self.tickers)):\n",
    "            delta_grid_t_i, tau_grid_t_i, IV_i, price_i = Data_per_Ticker(nu_pred[i*seq_length:(i+1)*seq_length], i)\n",
    "            IV_df = pd.DataFrame(data=np.array(IV_i)[0,0,:,:], columns=self.trained_model.data[0]['Delta']*100, index=self.trained_model.data[0]['tau']*365)\n",
    "            df_unpivoted = IV_df.stack().reset_index()\n",
    "            df_unpivoted.columns = ['days', 'delta', 'predicted_IV']\n",
    "            df_unpivoted['date'] = self.data[0]['dates'][start]\n",
    "            df_unpivoted['ticker'] = self.trained_model.tickers[i]\n",
    "            df_unpivoted['price'] = price_i[0]\n",
    "            df_unpivoted['pred_days'] = dT * 365\n",
    "            df_list.append(df_unpivoted)\n",
    "                    \n",
    "        return pd.concat(df_list, axis=0)\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FuNVol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mg:\\gitCode\\BU_programming\\finnal_project\\FuNVol\\predict.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/gitCode/BU_programming/finnal_project/FuNVol/predict.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# perform FPC projection\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/gitCode/BU_programming/finnal_project/FuNVol/predict.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mG:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mgitCode\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mBU_programming\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mfinnal_project\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFuNVol\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdynamicIV.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m md:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/gitCode/BU_programming/finnal_project/FuNVol/predict.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     trained_model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(md)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/gitCode/BU_programming/finnal_project/FuNVol/predict.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m Prediction(\u001b[39m'\u001b[39m\u001b[39mself.test_data/\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39m'\u001b[39m\u001b[39mAMZN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIBM\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mINTC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m], model\u001b[39m=\u001b[39mtrained_model, K\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\dill\\_dill.py:287\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, ignore, **kwds)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(file, ignore\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m    282\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[39m    Unpickle an object from a file.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \n\u001b[0;32m    285\u001b[0m \u001b[39m    See :func:`loads` for keyword arguments.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m Unpickler(file, ignore\u001b[39m=\u001b[39;49mignore, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\dill\\_dill.py:442\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     obj \u001b[39m=\u001b[39m StockUnpickler\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39mgetattr\u001b[39m(_main_module, \u001b[39m'\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    444\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ignore:\n\u001b[0;32m    445\u001b[0m             \u001b[39m# point obj class to main\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\storage.py:337\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[1;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1027\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1028\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:1256\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1254\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1255\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1256\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1258\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1260\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:1193\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1193\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[0;32m   1194\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1195\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1196\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m typed_storage\n\u001b[0;32m   1197\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    380\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 381\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    382\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    273\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 274\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    275\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    276\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32mc:\\Users\\Kishore\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    255\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    259\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    260\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    261\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    262\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    263\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# perform FPC projection\n",
    "with open(\"G:\\gitCode\\BU_programming\\\\finnal_project\\FuNVol\\data\\dynamicIV.pkl\", \"rb\") as md:\n",
    "    trained_model = pickle.load(md)\n",
    "model = Prediction('self.test_data/', ['AMZN', 'IBM', 'INTC', 'TSLA'], model=trained_model, K=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Generate data for future scenarios\n",
    "# Can provide the number of independent scenarios and time steps as arguments to the function\n",
    "\n",
    "with torch.no_grad():\n",
    "    delta_grid_t, tau_grid_t, IV, price, b_sim = model.Generate_Data(nsims=10, nsteps=31)\n",
    "    \n",
    "\"\"\" delta_grid_t and tau_grid_t are lists containing the transformed delta-tau meshgrid for each equity\n",
    "By default, IV values are calculated on the same grid as that for which data was available\n",
    "IV is a list containing the implied vol values on the above grid for each equity\n",
    "price is a list containing price paths for each equity\n",
    "b_sim contains the simulated time series of FPCCs and equity prices \"\"\"\n",
    "\n",
    "\"\"\" b_sim is nsims x nsteps x 36 (4 equities times 8 FPCCs plus 1 price) dimensional\n",
    "The first index corresponds to the generated (independent) scenarios\n",
    "The second index corresponds to the sequence of days where day 0 is the last day of training (observed) and the remaining 29 days are generated coefficients\n",
    "Hence day 0 will give the FPC coefficients (FPCCs) for the IV surface that is observed on the last day of training and is not a synthetic generated surface\n",
    "The last index corresponds to the different assets' FPCCs and transformed equity prices, details for third index below:\n",
    "0-7 give FPCCs for AMZN, 8 gives transformed equity price for AMZN\n",
    "9-16 give FPCCs for IBM, 17 gives transformed equity price for IBM\n",
    "18-25 give FPCCs for INTC, 26 gives transformed equity price for INTC\n",
    "27-34 give FPCCs for TSLA, 35 gives transformed equity price for TSLA \"\"\"\n",
    "\n",
    "\"\"\" Each element of price is nsims x nsteps dimensional \"\"\"\n",
    "\n",
    "\"\"\"Each element in the list IV is nsims x nsteps x len(tau) x len(delta) dimensional\n",
    "Each element of delta_grid_t and tau_grid_t is len(tau) x len(delta) containing the transformed\n",
    "values of delta and tau on a grid at which IV is calculated\"\"\"\n",
    "\n",
    "# to plot surfaces and get IV values at a different grid, refer to generated_data.ipynb\n",
    "with open('data/sim_data_coeffs.npy', 'wb') as f:\n",
    "    np.save(f, b_sim)\n",
    "    \n",
    "with open('data/sim_data_prices.npy', 'wb') as f:\n",
    "    np.save(f, np.array(price).transpose(1,2,0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
